{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce14WniT6BL4"
   },
   "source": [
    "# **Photovoltaic panel segmentation on building facades**\n",
    "\n",
    "Ayca Duran*, Pedram Mirabian, Panagiotis Karapiperis, Christoph Waibel,\n",
    "Bernd Bickel and Arno Schlueter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dataset Preparation\n",
    "\n",
    "Following the data collection phase, the images were pre-processed to ensure a high dataset quality. These steps were performed using the [roboflow](https://roboflow.com) platform to speed up collaboration. \n",
    "\n",
    "Following this step, a processing step was performed on the masks to include occlusions and remove non-PV areas that could not be easily labeled on Roboflow (patches of non-PV pixels within a PV mask).\n",
    "\n",
    "Finally, the \n",
    "\n",
    "TODO IMAGE ROBOFLOW MAINSCREEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Cleaning\n",
    "\n",
    "the images were manually inspected and cropped in some cases to remove non-essential environment features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Labeling\n",
    "\n",
    "Manual labeling was performed to determine the ground truth PV masks on the building facades and creating the dataset.\n",
    "\n",
    "The cleaned and labeled images were then split into training, validation and test sets using a stratified sampling approach (detailed below) utilizing the metadata of the PV projects to ensure a representative distribution in the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "The training dataset was then augmented by the following methods to increase the dataset size to 7x the original:\n",
    "\n",
    "* Flip\n",
    "* Crop (0% ... 40% zoom)\n",
    "* Saturation (-20% ... +20%)\n",
    "* Brightness (-20% ... +20%)\n",
    "* Exposure (-20% ... +20%)\n",
    "\n",
    "The labeling and augmentation step was performed using https://roboflow.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Masks\n",
    "\n",
    "To deal with holes in masks, using Detectron2 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GLOBAL SETUP\n",
    "'''\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount= False)\n",
    "    repo_path = Path(\"/content/drive/MyDrive/PVFINDER\")\n",
    "except:\n",
    "    repo_path = Path.cwd().parent\n",
    "    raise EnvironmentError(\"Google Colab environment not detected. Please run this notebook in Google Colab to ensure all required workflows are functional.\")\n",
    "\n",
    "base_path = repo_path / \"files\" / \"02_datasets\" / \"dataset\" / \"02_stratAug\" # should continue like dataset/02_stratAug/train/... ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title installation & imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import distutils.core\n",
    "\n",
    "# Check if GPU is available\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "!nvidia-smi\n",
    "\n",
    "# Detectron2 (needs download on Google Colab)\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "#\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "#\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine.hooks import HookBase\n",
    "#\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "#\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "#\n",
    "from detectron2.config import get_cfg\n",
    "#\n",
    "import detectron2.utils\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "setup_logger()\n",
    "import detectron2.utils.comm as comm\n",
    "#\n",
    "from detectron2 import structures\n",
    "\n",
    "# Logging\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Required libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pycocotools import mask as mask_utils\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# PyTorch (pre-installed on Google Colab)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "!nvcc --version\n",
    "TORCH_VERSION = (torch.__version__)\n",
    "print(\"torch: \", TORCH_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52e7ac30937440bb2af29c3e9cf2e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading image sizes...:   0%|          | 0/825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff9933c4cd74afc9236c6a28b9c2141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing masks...:   0%|          | 0/825 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /content/drive/MyDrive/DLforPVFacades/paper/dataset/7_dataset_final_PVandNonPV_NoAug_Unstratified/train/_annotations_case2.coco.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030a16cbf9234b6f8a61a5dde9ea0be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading image sizes...:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0155db8f064397adc48960c5e60ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing masks...:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /content/drive/MyDrive/DLforPVFacades/paper/dataset/7_dataset_final_PVandNonPV_NoAug_Unstratified/valid/_annotations_case2.coco.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1472ed2de6242ce80a12e7629daf7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading image sizes...:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2322f5db6a4f0182b4a89c9a3bdc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing masks...:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /content/drive/MyDrive/DLforPVFacades/paper/dataset/7_dataset_final_PVandNonPV_NoAug_Unstratified/test/_annotations_case2.coco.json\n"
     ]
    }
   ],
   "source": [
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "\n",
    "import sys, os, distutils.core\n",
    "\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "# Properly install detectron2. (Please do not install twice in both ways)\n",
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "\n",
    "import numpy as np\n",
    "import math, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "import os, json, pickle\n",
    "\n",
    "import torch\n",
    "import detectron2\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "!nvcc --version\n",
    "TORCH_VERSION = (torch.__version__)\n",
    "print(\"torch: \", TORCH_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)\n",
    "\n",
    "\n",
    "#@title CREATE CASE 2) find only visible PV - uncomment if needed to run\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools import mask as mask_utils\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Create binary masks for each class\n",
    "def get_mask(annotations, img_id, height, width, category_id):\n",
    "    \"\"\"Generates a binary mask for a given image and category.\"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for ann in annotations:\n",
    "        if ann['image_id'] == img_id and ann['category_id'] == category_id:\n",
    "            rles = mask_utils.frPyObjects(ann['segmentation'], height, width)  # FIX: Correct order (height, width)\n",
    "            decoded_mask = mask_utils.decode(rles)\n",
    "            if decoded_mask is not None:\n",
    "                decoded_mask = decoded_mask.reshape((height, width))  # Ensure correct shape\n",
    "                mask |= np.max(decoded_mask, axis=-1) if decoded_mask.ndim == 3 else decoded_mask\n",
    "    return mask\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    json_path = os.path.join(base_path, split, \"_annotations.coco.json\")\n",
    "    image_sizes_path = os.path.join(base_path, split, \"image_sizes.json\")\n",
    "    output_path = os.path.join(base_path, split, \"_annotations_case2.coco.json\")\n",
    "\n",
    "    # Load the COCO JSON data\n",
    "    with open(json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    image_sizes = {}\n",
    "    if os.path.exists(image_sizes_path):\n",
    "        try:\n",
    "            with open(image_sizes_path, 'r') as f:\n",
    "                image_sizes = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"WARNING: Could not read {image_sizes_path}. Resetting image sizes.\")\n",
    "            image_sizes = {}\n",
    "\n",
    "    if not image_sizes:\n",
    "        progress_bar = tqdm(total=len(coco_data[\"images\"]), desc=\"Reading image sizes...\", dynamic_ncols=True)\n",
    "        for img in coco_data['images']:\n",
    "            img_path = os.path.join(base_path, split, img['file_name'])\n",
    "            if os.path.exists(img_path):\n",
    "                with Image.open(img_path) as image:\n",
    "                    width, height = image.size\n",
    "                    image_sizes[img['file_name']] = {\"width\": width, \"height\": height}\n",
    "            else:\n",
    "                print(f\"ERROR: Image file {img_path} not found!\")\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        with open(image_sizes_path, 'w') as f:\n",
    "            json.dump(image_sizes, f, indent=2)\n",
    "        progress_bar.close()\n",
    "    else:\n",
    "        print(f\">>> Read {image_sizes_path}\\n{len(image_sizes)} items.\")\n",
    "\n",
    "    for img in coco_data[\"images\"]:\n",
    "        width, height = image_sizes[img[\"file_name\"]][\"width\"], image_sizes[img[\"file_name\"]][\"height\"]\n",
    "        if img['width'] != width or img['height'] != height:\n",
    "            print(f\"WARNING: Size mismatch in {img['file_name']} (COCO: {img['width']}x{img['height']}, Actual: {width}x{height})\")\n",
    "            img['width'], img['height'] = width, height  # Correct the values\n",
    "\n",
    "\n",
    "    # Get COCO class IDs dynamically\n",
    "    category_mapping = {cat['name']: cat['id'] for cat in coco_data['categories']}\n",
    "    PV_ID = category_mapping.get(\"pv\", None)\n",
    "    PV_OCCLUDED_ID = category_mapping.get(\"pv_occluded\", None)\n",
    "    NONPV_FACADE_ID = category_mapping.get(\"nonpv_facade\", None)\n",
    "\n",
    "    if None in [PV_ID, PV_OCCLUDED_ID, NONPV_FACADE_ID]:\n",
    "        raise ValueError(\"One or more required categories are missing from COCO JSON\")\n",
    "\n",
    "    new_annotations = []\n",
    "    ann_id = 0\n",
    "\n",
    "    progress_bar = tqdm(total=len(coco_data[\"images\"]), desc=\"Processing masks...\", dynamic_ncols=True)\n",
    "    for img in coco_data['images']:\n",
    "        img_id = img[\"id\"]\n",
    "        width, height = image_sizes[img['file_name']][\"width\"], image_sizes[img['file_name']][\"height\"]\n",
    "        img_path = os.path.join(base_path, split, img['file_name'])\n",
    "\n",
    "        pv_mask = get_mask(coco_data['annotations'], img_id, height, width, PV_ID)\n",
    "        occlusion_mask = get_mask(coco_data['annotations'], img_id, height, width, PV_OCCLUDED_ID)\n",
    "        nonpv_mask = get_mask(coco_data['annotations'], img_id, height, width, NONPV_FACADE_ID)\n",
    "\n",
    "        shared_occlusion = np.logical_and(pv_mask, occlusion_mask).astype(np.uint8)\n",
    "        # cleaned_pv_mask = np.clip(pv_mask - shared_occlusion - nonpv_mask, 0, 1)\n",
    "        cleaned_pv_mask = pv_mask.copy()\n",
    "        cleaned_pv_mask[shared_occlusion == 1] = 0  # Remove only the overlapping occlusions\n",
    "        cleaned_pv_mask[nonpv_mask == 1] = 0  # Remove NonPV areas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # Define kernel for morphological operations\n",
    "        # kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "        # # Apply morphological opening to remove small noise & thin lines\n",
    "        # cleaned_pv_mask = cv2.morphologyEx(cleaned_pv_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # # Remove small connected components\n",
    "        # num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(cleaned_pv_mask, connectivity=8)\n",
    "\n",
    "        # # Set a minimum area threshold (adjust as needed)\n",
    "        # min_area = 20  # Increase this if needed\n",
    "        # for i in range(1, num_labels):  # Ignore background (0)\n",
    "        #     if stats[i, cv2.CC_STAT_AREA] < min_area:\n",
    "        #         cleaned_pv_mask[labels == i] = 0  # Remove small components\n",
    "\n",
    "        # # **Optional: Apply morphological closing to fill gaps left by thin line removal**\n",
    "        # cleaned_pv_mask = cv2.morphologyEx(cleaned_pv_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # # **Optional: Additional Thinning Removal**\n",
    "        # # Detect edges using Sobel to find thin structures\n",
    "        # edges = cv2.Sobel(cleaned_pv_mask, cv2.CV_8U, 1, 1, ksize=3)\n",
    "        # thin_lines = (edges > 0).astype(np.uint8)\n",
    "\n",
    "        # # Subtract detected thin lines from the cleaned mask\n",
    "        # cleaned_pv_mask = np.clip(cleaned_pv_mask - thin_lines, 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Visualization\n",
    "        # fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "        # original_image = np.array(Image.open(img_path).convert(\"RGB\")) if os.path.exists(img_path) else np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        # axes[0].imshow(original_image)\n",
    "        # axes[0].set_title(\"Original Image\")\n",
    "        # axes[1].imshow(pv_mask, cmap=\"Reds\", alpha=0.7)\n",
    "        # axes[1].set_title(\"PV Mask\")\n",
    "        # axes[2].imshow(occlusion_mask, cmap=\"Blues\", alpha=0.7)\n",
    "        # axes[2].set_title(\"PV Occluded\")\n",
    "        # axes[3].imshow(nonpv_mask, cmap=\"Greens\", alpha=0.7)\n",
    "        # axes[3].set_title(\"NonPV Facade\")\n",
    "        # axes[4].imshow(cleaned_pv_mask, cmap=\"gray\")\n",
    "        # axes[4].set_title(\"Final PV Mask\")\n",
    "\n",
    "        # for ax in axes:\n",
    "        #     ax.axis(\"off\")\n",
    "        # plt.show()\n",
    "\n",
    "        # Find connected components to separate PV regions\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(cleaned_pv_mask.astype(np.uint8), connectivity=8)\n",
    "\n",
    "        for i in range(1, num_labels):  # Ignore the background label (0)\n",
    "            new_mask = (labels == i).astype(np.uint8)\n",
    "            bbox_width = int(stats[i, cv2.CC_STAT_WIDTH])\n",
    "            bbox_height = int(stats[i, cv2.CC_STAT_HEIGHT])\n",
    "            if new_mask.sum() > 0 and bbox_width >= 10 and bbox_height >= 10:  # Filter small areas\n",
    "                rle = mask_utils.encode(np.asfortranarray(new_mask))\n",
    "                rle['counts'] = rle['counts'].decode('utf-8')  # Convert bytes to string for JSON compatibility\n",
    "                new_annotations.append({\n",
    "                    'id': ann_id,\n",
    "                    'image_id': img_id,\n",
    "                    'category_id': PV_ID,\n",
    "                    'segmentation': rle,\n",
    "                    'area': int(stats[i, cv2.CC_STAT_AREA]),\n",
    "                    'bbox': [\n",
    "                        int(stats[i, cv2.CC_STAT_LEFT]),\n",
    "                        int(stats[i, cv2.CC_STAT_TOP]),\n",
    "                        bbox_width,\n",
    "                        bbox_height\n",
    "                    ],\n",
    "                    'iscrowd': 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Update COCO JSON\n",
    "    coco_data[\"images\"] = coco_data[\"images\"] # the dimensions (if needed) have been changed in the code\n",
    "    coco_data['annotations'] = new_annotations\n",
    "    coco_data['categories'] = [cat for cat in coco_data['categories'] if cat['id'] == PV_ID]\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "\n",
    "    print(f\"Processed and saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Masks for Occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title def functions - just run this\n",
    "\n",
    "def make_next(models_dir, d2model, d2epochs, d2lr, d2bs, d2wd, make= True) -> str:\n",
    "    # Calculate model iteration\n",
    "    models_dir_models = [item for item in os.listdir(models_dir) if item[0] != \".\"]\n",
    "    if len(models_dir_models) != 0:\n",
    "        nums = [text.split(\"_\")[0] for text in models_dir_models]\n",
    "\n",
    "        models = sorted(models_dir_models, key=lambda folder: int(folder.split('_')[0]))\n",
    "        model_num = int(models[-1].split(\"_\")[0]) + 1\n",
    "    else:\n",
    "        model_num = 0\n",
    "\n",
    "    rem = 3 - len(str(model_num))\n",
    "    zeroes = \"0\" * rem\n",
    "    model_arch = d2model.split(\"/\")[1]\n",
    "    model_name = f\"{zeroes}{model_num}_{model_arch}_{d2epochs}epochs_{d2lr}lr_{d2bs}bs_{d2wd}wd\"\n",
    "\n",
    "    model_dir = os.path.join(models_dir, model_name)\n",
    "    if make:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        print(f\">>> created model folder at\\n{model_dir}\")\n",
    "    else:\n",
    "        print(f\">>> if make == true then folder will be created at\\n{model_dir}\")\n",
    "\n",
    "    return model_dir\n",
    "\n",
    "def model_name_from_num(model_num):\n",
    "    zeroes = \"0\" * (3 - len(str(model_num)))\n",
    "    model_name = None\n",
    "\n",
    "    for folder in os.listdir(trained_models_folder):\n",
    "        if folder.split(\"_\")[0] == f\"{zeroes}{model_num}\":\n",
    "            model_name = folder\n",
    "    if model_name == None:\n",
    "        raise Exception(f\"Model with number {model_num} not found in\\n{trained_models_folder}\")\n",
    "\n",
    "    return model_name\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "\n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):\n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop\n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "\n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_starting = next_iter == 5\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_starting or is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "\n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks\n",
    "\n",
    "class MeanStdDataset(Dataset):\n",
    "    def __init__(self, data_dir, pv_only= False, extension= '.jpg', transform= None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_files  = [f for f in os.listdir(data_dir) if f.endswith(extension)]\n",
    "        if pv_only:\n",
    "            self.image_files = [f for f in self.image_files if \"_gsv_\" in f or \"_web_\" in f]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Open the image and convert to a numpy array, then to a torch tensor.\n",
    "        image = torch.from_numpy(np.array(PIL.Image.open(os.path.join(self.data_dir, self.image_files[index])), dtype=np.float32))\n",
    "        # If grayscale, add a channel dimension\n",
    "        if image.ndim == 2:\n",
    "            image = image.unsqueeze(-1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def compute_mean_std(dataloader):\n",
    "    '''\n",
    "    Compute the per-channel mean and standard deviation for images that may have different sizes.\n",
    "    This function iterates over images (even if in batches) and computes the running sum, squared sum and total pixel count.\n",
    "    '''\n",
    "    total_sum = None\n",
    "    total_sq_sum = None\n",
    "    total_pixels = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Each batch is a list of images (they are not stacked since their sizes vary)\n",
    "        for image in batch:\n",
    "            # image shape: (H, W, C) or (H, W) - but we've handled grayscale in __getitem__\n",
    "            # Compute per-channel sum and squared sum over height and width.\n",
    "            # If image is (H, W, C), then summing over dimensions 0 and 1 gives a vector of length C.\n",
    "            img_sum = torch.sum(image, dim=[0, 1])\n",
    "            img_sq_sum = torch.sum(image ** 2, dim=[0, 1])\n",
    "            # Total number of pixels per channel in this image (same for each channel)\n",
    "            num_pixels = image.shape[0] * image.shape[1]\n",
    "\n",
    "            if total_sum is None:\n",
    "                total_sum = img_sum\n",
    "                total_sq_sum = img_sq_sum\n",
    "            else:\n",
    "                total_sum += img_sum\n",
    "                total_sq_sum += img_sq_sum\n",
    "\n",
    "            total_pixels += num_pixels\n",
    "\n",
    "    # The mean is computed by dividing the total per-channel sum by the total number of pixels.\n",
    "    mean = (total_sum / total_pixels)\n",
    "    # Standard deviation: sqrt(E[X^2] - (E[X])^2)\n",
    "    std = torch.sqrt(total_sq_sum / total_pixels - mean ** 2)\n",
    "    return mean, std\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return batch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.meshgrid: in an upcoming release\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Mean and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title calculate dataset mean std - and the resulting numbers\n",
    "\n",
    "calculated_values = {\n",
    "    5: {\n",
    "        \"mean\": [122.1287, 129.9507, 136.4666],\n",
    "        \"std\": [72.9773, 72.1729, 77.3321]\n",
    "    }\n",
    "}\n",
    "\n",
    "# The above values are calculated using the method below.\n",
    "# If you want to calculate them again (this might take a while),\n",
    "# toggle the boolean below:\n",
    "calc_mean_std = False\n",
    "\n",
    "if calc_mean_std:\n",
    "    # calculate mean and std for the selected dataset\n",
    "    dataset = MeanStdDataset(os.path.join(dataset_dir, \"train\"), pv_only= pv_only)\n",
    "    print(f\"{len(dataset)} images... (progress bar is batch size 4)\")\n",
    "    train_loader = DataLoader(dataset, batch_size=4, shuffle=False,\n",
    "                                num_workers=2, collate_fn=custom_collate, pin_memory=True)\n",
    "    total_mean, total_std = compute_mean_std(train_loader)\n",
    "    print(f'\\nmean (RGB):', total_mean)\n",
    "    print('std (RGB): ', total_std)\n",
    "else:\n",
    "    # use pre-calculated values based on the selected dataset\n",
    "    total_mean = calculated_values[5][\"mean\"]\n",
    "    total_std = calculated_values[5][\"std\"]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
