{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12945,
     "status": "ok",
     "timestamp": 1759500271441,
     "user": {
      "displayName": "Pedram Mirabian",
      "userId": "13518418274543203866"
     },
     "user_tz": -120
    },
    "id": "bJ8h4Br2BNaN",
    "outputId": "ae1accaa-2f58-4b5d-ad55-db16598a6a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25782,
     "status": "ok",
     "timestamp": 1759500297230,
     "user": {
      "displayName": "Pedram Mirabian",
      "userId": "13518418274543203866"
     },
     "user_tz": -120
    },
    "id": "eeKte9vHpbVm",
    "outputId": "16fb1cc4-6aa2-41f8-eadc-146a1257df6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n",
      "Cloning into 'detectron2'...\n",
      "remote: Enumerating objects: 15912, done.\u001b[K\n",
      "remote: Total 15912 (delta 0), reused 0 (delta 0), pack-reused 15912 (from 1)\u001b[K\n",
      "Receiving objects: 100% (15912/15912), 6.67 MiB | 16.69 MiB/s, done.\n",
      "Resolving deltas: 100% (11332/11332), done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1003 14:04:38.186000 1210 torch/utils/cpp_extension.py:118] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (2.0.10)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black\n",
      "  Downloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from yacs>=0.1.8) (6.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.1) (4.9.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black) (8.2.1)\n",
      "Collecting mypy-extensions>=0.4.3 (from black)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black) (4.4.0)\n",
      "Collecting pytokens>=0.1.10 (from black)\n",
      "  Downloading pytokens-0.1.10-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading black-25.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytokens-0.1.10-py3-none-any.whl (12 kB)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: fvcore\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=c5fa4939bb814416ce3c8fce97e37aee1e3bd7d872bcfdaf30a8bba9b68173a0\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
      "Successfully built fvcore\n",
      "Installing collected packages: yacs, pytokens, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore\n",
      "Successfully installed black-25.9.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.2.0 pytokens-0.1.10 yacs-0.1.8\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "Build cuda_12.5.r12.5/compiler.34385749_0\n",
      "torch:  2.8.0+cu126\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "# check to see if GPU\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "!nvidia-smi\n",
    "\n",
    "import sys, os, distutils.core\n",
    "\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "import numpy as np\n",
    "import math, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "import os, json, pickle\n",
    "\n",
    "import torch\n",
    "import detectron2\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "!nvcc --version\n",
    "TORCH_VERSION = (torch.__version__)\n",
    "print(\"torch: \", TORCH_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "error",
     "timestamp": 1759500297249,
     "user": {
      "displayName": "Pedram Mirabian",
      "userId": "13518418274543203866"
     },
     "user_tz": -120
    },
    "id": "GYedPcLks0uY",
    "outputId": "18d3589c-80fa-4c70-fe6c-a73bbb4de5c0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DLforPVFacades/paper/dataset/5_dataset_final_PVandNonPV_ColorAug/PVFinderPaper.v7-pvandnonpv_augflipcropcolor.coco-segmentation/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1880319883.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mregister_coco_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_annotations_case2.coco.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{split} with {len([item for item in os.listdir(os.path.join(base_path, split)) if item.endswith('.jpg')])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DLforPVFacades/paper/dataset/5_dataset_final_PVandNonPV_ColorAug/PVFinderPaper.v7-pvandnonpv_augflipcropcolor.coco-segmentation/test'"
     ]
    }
   ],
   "source": [
    "#@title load images\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "# base_path = \"/content/drive/MyDrive/DLforPVFacades/paper/dataset/4_dataset_final_PVandNonPV/PVFinderPaper.v3-pvandnonpv.coco-segmentation/\"\n",
    "base_path = \"/content/drive/MyDrive/DLforPVFacades/paper/dataset/5_dataset_final_PVandNonPV_ColorAug/PVFinderPaper.v7-pvandnonpv_augflipcropcolor.coco-segmentation\"\n",
    "\n",
    "for split in [\"test\"]:\n",
    "    try:\n",
    "        MetadataCatalog.remove(split)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        DatasetCatalog.remove(split)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    register_coco_instances(split, {}, os.path.join(base_path, split, \"_annotations_case2.coco.json\"), os.path.join(base_path, split))\n",
    "    print(f\"{split} with {len([item for item in os.listdir(os.path.join(base_path, split)) if item.endswith('.jpg')])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0Xrs8v-BGps"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pprint\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2 import structures\n",
    "from pycocotools import mask as mask_utils\n",
    "import csv\n",
    "import gc\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "110fgne5eglVxeXXZhhuDYBYwxQ8podn_"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 186060,
     "status": "error",
     "timestamp": 1744635892118,
     "user": {
      "displayName": "Pedram Mirabian",
      "userId": "13518418274543203866"
     },
     "user_tz": -120
    },
    "id": "H8TzhuyypbVr",
    "outputId": "8865cd3c-0c9b-488d-f951-9374dfd1d6d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "csv_file = '/content/drive/MyDrive/DLforPVFacades/paper/human_labeling/Batch_5301916_batch_results_fin.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "output_folder = '/content/drive/MyDrive/DLforPVFacades/paper/human_labeling/output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "masks_folder = '/content/drive/MyDrive/DLforPVFacades/paper/human_labeling/masks'\n",
    "os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "dataset = 'test'\n",
    "dataset_dicts = DatasetCatalog.get(dataset)\n",
    "metadata = MetadataCatalog.get(dataset)\n",
    "\n",
    "gt_dict = {}\n",
    "for item in dataset_dicts:\n",
    "    img_name = os.path.basename(item[\"file_name\"]).split(\".rf\")[0]\n",
    "    gt_dict[img_name] = item\n",
    "\n",
    "results = []\n",
    "for image_url, group in df.groupby('Input.image_url'):\n",
    "    print(f\"\\n\\n\\n>>> img: {image_url}\")\n",
    "\n",
    "\n",
    "    '''\n",
    "    LOAD GROUND TRUTH IMAGE\n",
    "    '''\n",
    "    img_name = image_url.split('.rf')[0]\n",
    "    gt = gt_dict[img_name]\n",
    "\n",
    "    print(f\"h: {gt['height']}\\tw:{gt['width']}\")\n",
    "    print(len(gt['annotations']))\n",
    "\n",
    "    img = cv2.imread(gt[\"file_name\"])\n",
    "\n",
    "    # CREATE UNIFIED GT MASK\n",
    "    gt_mask = None\n",
    "    gt_union_mask = np.zeros_like(img[:, :, 0], dtype=int)\n",
    "    for item in gt['annotations']:\n",
    "        gt_mask = mask_utils.decode(item['segmentation']).astype(int)\n",
    "        gt_union_mask = np.logical_or(gt_union_mask, gt_mask).astype(int)\n",
    "    gt_mask = gt_union_mask\n",
    "\n",
    "    '''\n",
    "    BRING EACH WORKER'S ANNOTATION MASK (ANNOT)\n",
    "    '''\n",
    "    annotations = {}\n",
    "\n",
    "    # for each worker, extract annotations\n",
    "    for idx, row in group.iterrows():\n",
    "        if row['AssignmentStatus'] != 'Approved':\n",
    "            print(f\"\\n\\n\\nSKIPPING {row}\")\n",
    "            continue\n",
    "        worker_id = row['WorkerId']\n",
    "        polygons_data = row['Answer.annotatedResult.polygons']\n",
    "        if pd.isna(polygons_data) or polygons_data == '[]':  # No annotation case\n",
    "            annotations[worker_id] = None\n",
    "        else:\n",
    "            annotations[worker_id] = json.loads(polygons_data)\n",
    "\n",
    "\n",
    "\n",
    "    for worker, polygons in annotations.items():\n",
    "        polygons_vertices = []\n",
    "\n",
    "        print(worker, polygons)\n",
    "        if polygons is not None:\n",
    "            for polygon in polygons:\n",
    "                vertices = [(vertex['x'], vertex['y']) for vertex in polygon['vertices']]\n",
    "                polygons_vertices.append(vertices)  # <-- This was wrongly indented\n",
    "\n",
    "        pred_mask = np.zeros_like(img[:, :, 0], dtype=np.uint8)  # should be uint8 for cv2\n",
    "        for polygon_vertices in polygons_vertices:\n",
    "            pts = np.array([polygon_vertices], dtype=np.int32)  # must be wrapped in list\n",
    "            cv2.fillPoly(pred_mask, pts, color=1)\n",
    "\n",
    "        # Define the save directory\n",
    "        img_folder_name = os.path.splitext(os.path.basename(image_url))[0]  # Remove .jpg\n",
    "        save_dir = os.path.join(masks_folder, img_folder_name)\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Define the path for this worker's mask\n",
    "        mask_filename = f\"{worker}.png\"  # or .jpg if you prefer\n",
    "        mask_path = os.path.join(save_dir, mask_filename)\n",
    "\n",
    "        print()\n",
    "\n",
    "        # Save the mask\n",
    "        cv2.imwrite(mask_path, (pred_mask * 255).astype(np.uint8))  # Save mask as binary image (0 or 255)\n",
    "\n",
    "        # Convert BGR to RGB for Matplotlib\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).copy()\n",
    "\n",
    "        # Create base transparent background by dimming the image\n",
    "        transparent_bg = (img_rgb * 0.2).astype(np.uint8)  # 20% brightness\n",
    "\n",
    "        # Create overlay to add masks\n",
    "        overlay = transparent_bg.copy()\n",
    "\n",
    "        # Define colors\n",
    "        gt_color = (255, 0, 0)          # Red\n",
    "        pred_color = (255, 165, 0)      # Orange\n",
    "        overlap_color = (0, 255, 0)     # Green\n",
    "\n",
    "        # Overlay masks\n",
    "        overlay[gt_mask == 1] = gt_color\n",
    "        overlay[pred_mask == 1] = pred_color\n",
    "        overlay[(gt_mask == 1) & (pred_mask == 1)] = overlap_color\n",
    "\n",
    "        # Blend with the transparent base (which is already dimmed, so blending is optional)\n",
    "        blended = overlay\n",
    "\n",
    "        # Show both images side by side\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        axes[0].imshow(img_rgb)\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(blended)\n",
    "        axes[1].set_title(f\"Worker {worker} vs GT\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # DETECTION-LEVEL METRICS\n",
    "        gt_detected = gt_mask.sum() > 0\n",
    "        pred_detected = pred_mask.sum() > 0\n",
    "\n",
    "        if gt_detected and pred_detected:\n",
    "            det_category = 'TP'\n",
    "        elif not gt_detected and pred_detected:\n",
    "            det_category = 'FP'\n",
    "        elif gt_detected and not pred_detected:\n",
    "            det_category = 'FN'\n",
    "        else:\n",
    "            det_category = 'TN'\n",
    "\n",
    "        # Calculate PIXEL-LEVEL TP, FP, FN, TN\n",
    "        tp = np.logical_and(gt_mask == 1, pred_mask == 1).sum()\n",
    "        fp = np.logical_and(gt_mask == 0, pred_mask == 1).sum()\n",
    "        fn = np.logical_and(gt_mask == 1, pred_mask == 0).sum()\n",
    "        tn = np.logical_and(gt_mask == 0, pred_mask == 0).sum()\n",
    "\n",
    "        # Optionally calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        iou = tp / (tp + fp + fn)\n",
    "\n",
    "        print(f\"\\nMetrics for Worker {worker}:\")\n",
    "        print(f\"TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
    "        print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}, IoU: {iou:.3f}\")\n",
    "\n",
    "        # Inside loop per image & worker:\n",
    "        results.append({\n",
    "            'image': image_url,\n",
    "            'worker': worker,\n",
    "            'det_category': det_category,\n",
    "            'img_precision': precision,\n",
    "            'img_recall': recall,\n",
    "            'img_f1': f1,\n",
    "            'img_iou': iou,\n",
    "            'TP_pixels': tp,\n",
    "            'FP_pixels': fp,\n",
    "            'FN_pixels': fn,\n",
    "            'TN_pixels': tn,\n",
    "            })\n",
    "\n",
    "# After the full loop:\n",
    "metrics_df = pd.DataFrame(results)\n",
    "metrics_df.to_csv(os.path.join(output_dir, 'humanVSai.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1eTWjtjmZz5j3jgUjz6yKtFlhOrPnYwbu",
     "timestamp": 1743952929877
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
